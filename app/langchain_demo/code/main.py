import os
import shutil
import torch
from app_modules.overwrites import postprocess
from app_modules.presets import *
from clc.langchain_application import LangChainApplication
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device1 = torch.device("cuda:0")  # ä½¿ç”¨ç¬¬ä¸€å¼ GPUå¡
device2 = torch.device("cuda:1")  # ä½¿ç”¨ç¬¬äºŒå¼ GPUå¡
# ä¿®æ”¹æˆè‡ªå·±çš„é…ç½®ï¼ï¼ï¼
class LangChainCFG:
    llm_model_name = '/root/data1/luwen/luwen_baichuan/output/zju_model_0710_100k_wenshu30k'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    llm_model_name2 = '/root/data1/luwen/luwen_llama/mymodel/alpaca_plus_13b_sft298k_hf'
    embedding_model_name = '/root/data1/luwen/luwen_llama/mymodel/text2vec_large'  # æ£€ç´¢æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    vector_store_path = 'scripts/langchain_demo/cache2/'
    docs_path = 'scripts/langchain_demo/test_docs'
    kg_vector_stores = {
        'åˆ‘æ³•æ³•æ¡': 'scripts/langchain_demo/cache2/legal_article',
        'åˆ‘æ³•ä¹¦ç±': 'scripts/langchain_demo/cache2/legal_book',
        'æ³•å¾‹æ–‡ä¹¦æ¨¡ç‰ˆ':'scripts/langchain_demo/cache2/legal_template',
        'åˆ‘æ³•æ¡ˆä¾‹': 'scripts/langchain_demo/cache2/legal_case',
        # 'åˆå§‹åŒ–': 'scripts/langchain_demo/cache',
    }  # å¯ä»¥æ›¿æ¢æˆè‡ªå·±çš„çŸ¥è¯†åº“ï¼Œå¦‚æœæ²¡æœ‰éœ€è¦è®¾ç½®ä¸ºNone
    # kg_vector_stores=None
    patterns = ['æ¨¡å‹é—®ç­”', 'çŸ¥è¯†åº“é—®ç­”']  #
    # n_gpus=1
    device = {"dev1":device1,"dev2":device2}



config = LangChainCFG()
application = LangChainApplication(config)

application.source_service.init_source_vector()

def get_file_list():
    if not os.path.exists("docs"):
        return []
    return [f for f in os.listdir("docs")]


file_list = get_file_list()


def upload_file(file):
    if not os.path.exists("docs"):
        os.mkdir("docs")
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
    file_list.insert(0, filename)
    application.source_service.add_document("docs/" + filename)
    return gr.Dropdown.update(choices=file_list, value=filename)


def set_knowledge(kg_names, history):
    kg_print_out = ""
    try:
        for kg_name in kg_names:
            application.source_service.load_vector_store(config.kg_vector_stores[kg_name])
            kg_print_out = kg_print_out + "  " + kg_name
        msg_status = f'{kg_print_out}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        print(e)
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]


def clear_session():
    return '', None


def predict(input,
            large_language_model,
            embedding_model,
            top_k,
            # use_web,
            use_pattern,
            kg_names,
            history=None,
            max_length=None):
    # print(large_language_model, embedding_model)
    if large_language_model=="zju-lm":
        application.llm_service.tokenizer = application.tokenizer2
        application.llm_service.model = application.model2
    application.llm_service.max_token = max_length
    # print(input)
    if history == None:
        history = []
    use_web = "ä¸ä½¿ç”¨"
    if use_web == 'ä½¿ç”¨':
        web_content = application.source_service.search_web(query=input)
    else:
        web_content = ''
    search_text = ''
    if use_pattern == 'æ¨¡å‹é—®ç­”':
        result = application.get_llm_answer(query=input, web_content=web_content,chat_history=history, model_name=large_language_model)
        history.append((input, result))
        search_text += web_content
        return '', history, history, search_text

    else:
        result, context_with_score = application.get_knowledge_based_answer(
            query=input,
            history_len=5,
            temperature=0.1,
            top_p=0.9,
            top_k=top_k,
            web_content=web_content,
            chat_history=history,
            kg_names = kg_names,
            model_name = large_language_model
        )
        history.append((input, result))
        search_text += context_with_score
        # history.append((input, resp['result']))
        # for idx, source in enumerate(resp['source_documents'][:4]):
        #     sep = f'----------ã€æœç´¢ç»“æœ{idx + 1}ï¼šã€‘---------------\n'
        #     search_text += f'{sep}\n{source.page_content}\n\n'
        # print(search_text)
        # search_text += "----------ã€ç½‘ç»œæ£€ç´¢å†…å®¹ã€‘-----------\n"
        # search_text += web_content
        return '', history, history, search_text


with open("scripts/langchain_demo/assets/custom.css", "r", encoding="utf-8") as f:
    customCSS = f.read()
# with gr.Blocks(css=customCSS, theme=small_and_beautiful_theme) as demo:
with gr.Blocks() as demo:    
    gr.Markdown("""<h1><center>æ™ºæµ·-å½•é—®</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):

            top_k = gr.Slider(1,
                              20,
                              value=4,
                              step=1,
                              label="æ£€ç´¢top-kæ–‡æ¡£",
                              interactive=True)

            # use_web = gr.Radio(["ä½¿ç”¨", "ä¸ä½¿ç”¨"], label="web search",
            #                    info="æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢ï¼Œä½¿ç”¨æ—¶ç¡®ä¿ç½‘ç»œé€šå¸¸",
            #                    value="ä¸ä½¿ç”¨"
            #                    )
            # use_web = "ä¸ä½¿ç”¨"
            use_pattern = gr.Radio(
                [
                    'æ¨¡å‹é—®ç­”',
                    'çŸ¥è¯†åº“é—®ç­”',
                ],
                label="æ¨¡å¼",
                value='æ¨¡å‹é—®ç­”',
                interactive=True)

            kg_names = gr.CheckboxGroup(list(config.kg_vector_stores.keys()),
                               label="çŸ¥è¯†åº“",
                               value=None,
                               info="ä½¿ç”¨çŸ¥è¯†åº“é—®ç­”ï¼Œè¯·åŠ è½½çŸ¥è¯†åº“",
                               interactive=True).style(height=200)
            set_kg_btn = gr.Button("åŠ è½½çŸ¥è¯†åº“")
            with gr.Row():
                gr.Markdown("""æé†’ï¼š<br>
                                        æ™ºæµ·-å½•é—®æ˜¯åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯æ„å»ºçš„ï¼Œå®ƒå¯ä»¥æä¾›æœ‰ä»·å€¼çš„æ³•å¾‹å»ºè®®å’Œè§£é‡Šï¼Œä½†ä¸åº”è§†ä¸ºæ³•å¾‹ä¸“å®¶çš„æ›¿ä»£å“ã€‚åœ¨é‡è¦çš„æ³•å¾‹äº‹åŠ¡ä¸­ï¼Œå»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®æˆ–å¾‹å¸ˆã€‚ <br>
                                        """)

            # file = gr.File(label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…",
            #                visible=True,
            #                file_types=['.txt', '.md', '.docx', '.pdf']
            #                )

        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(label='æ™ºæµ·-å½•é—®').style(height=300)
            with gr.Row():
                message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
            with gr.Row():
                clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                send = gr.Button("ğŸš€ å‘é€")
            # with gr.Row():
            #     gr.Markdown("""æé†’ï¼š<br>
            #                             å¸æ³•å¤§æ¨¡å‹-ZJUæ˜¯åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯æ„å»ºçš„ï¼Œå®ƒå¯ä»¥æä¾›æœ‰ä»·å€¼çš„æ³•å¾‹å»ºè®®å’Œè§£é‡Šï¼Œä½†ä¸åº”è§†ä¸ºæ³•å¾‹ä¸“å®¶çš„æ›¿ä»£å“ã€‚åœ¨é‡è¦çš„æ³•å¾‹äº‹åŠ¡ä¸­ï¼Œå»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®æˆ–å¾‹å¸ˆã€‚ <br>
            #                             """)
        with gr.Column(scale=2):
            embedding_model = gr.Dropdown([
                "text2vec-large"
            ],
                label="Embedding model",
                value="text2vec-large")

            large_language_model = gr.Dropdown(
                [
                    "zju-bc",
                    "zju-lm",
                ],
                label="large language model",
                value="zju-bc")
            max_length = gr.Slider(
                    0, 4096, value=1024, step=1.0, label="Maximum length", interactive=True)
        # with gr.Column(scale=2):
            search = gr.Textbox(label='æœç´¢ç»“æœ')
        
        # ============= è§¦å‘åŠ¨ä½œ=============
        # file.upload(upload_file,
        #             inputs=file,
        #             outputs=None)
        set_kg_btn.click(
            set_knowledge,
            show_progress=True,
            inputs=[kg_names, chatbot],
            outputs=chatbot
        )
        # å‘é€æŒ‰é’® æäº¤
        send.click(predict,
                   inputs=[
                       message,
                       large_language_model,
                       embedding_model,
                       top_k,
                    #    use_web,
                       use_pattern,
                       kg_names,
                       state,
                       max_length,
                   ],
                   outputs=[message, chatbot, state, search])

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(fn=clear_session,
                            inputs=[],
                            outputs=[chatbot, state],
                            queue=False)

        # è¾“å…¥æ¡† å›è½¦
        message.submit(predict,
                       inputs=[
                           message,
                           large_language_model,
                           embedding_model,
                           top_k,
                        #    use_web,
                           use_pattern,
                           kg_names,
                           state,
                           max_length
                       ],
                       outputs=[message, chatbot, state, search])

demo.queue(concurrency_count=2).launch(
    server_name='0.0.0.0',
    # server_port=8888,
    share=False,
    show_error=True,
    debug=True,
    enable_queue=True,
    inbrowser=True,
)
